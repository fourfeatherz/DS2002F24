{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIkex+8d4mg8dzy2gNDOz9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rixprakash/DS2002F24/blob/main/project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/rixprakash/DS2002F24"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4NfZ3zta_Rn",
        "outputId": "91b9b37c-feeb-4838-8391-144e481f6705"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DS2002F24'...\n",
            "remote: Enumerating objects: 366, done.\u001b[K\n",
            "remote: Counting objects: 100% (199/199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 366 (delta 126), reused 101 (delta 64), pack-reused 167 (from 1)\u001b[K\n",
            "Receiving objects: 100% (366/366), 17.60 MiB | 13.18 MiB/s, done.\n",
            "Resolving deltas: 100% (169/169), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import sqlite3\n",
        "import os\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/DS2002F24/dataproject1/Crime_Data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "dfbefore = df.copy() # Copies the original file\n",
        "\n",
        "# Function to add the new file for Exact Date Report\n",
        "def add_exact_date_report_column(df):\n",
        "    try:\n",
        "        # Extract the date part (first 10 characters) from 'DateReported'\n",
        "        df['ExactDateReport'] = df['DateReported'].str[:10]\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding 'ExactDateReport' column: {e}\")\n",
        "    return df\n",
        "\n",
        "# Function to split Officer Name\n",
        "def split_reporting_officer(df):\n",
        "    try:\n",
        "        # Split the 'ReportingOfficer' column by the comma\n",
        "        df[['OfficerLastName', 'OfficerFirstName']] = df['ReportingOfficer'].str.split(', ', expand=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error splitting 'ReportingOfficer' into 'OfficerFirstName' and 'OfficerLastName': {e}\") # If an error occurs\n",
        "    return df\n",
        "\n",
        "# Function to convert CSV file to JSON\n",
        "def convert_csv_to_json(df, file_path):\n",
        "    try:\n",
        "        json_data = df.to_json(orient='records')\n",
        "        json_file_path = file_path.replace('.csv', '.json') # Changing to json\n",
        "        with open(json_file_path, 'w') as json_file:\n",
        "            json_file.write(json_data)\n",
        "        print(f\"CSV file converted to JSON and saved as {json_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting CSV to JSON: {e}\")\n",
        "\n",
        "# Function to convert CSV file to SQL\n",
        "def convert_csv_to_sql(df, file_path, db_name='crime_data.db', table_name='crime_records'):\n",
        "    try:\n",
        "        # Connect to SQLite database (or create it if it doesn't exist)\n",
        "        conn = sqlite3.connect(db_name)\n",
        "\n",
        "        # Write the dataframe to the SQL database\n",
        "        df.to_sql(table_name, conn, if_exists='replace', index=False) # Stores as a SQL database\n",
        "        conn.close()\n",
        "        print(f\"CSV file converted to SQL and saved in {db_name} database, table: {table_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting CSV to SQL: {e}\")\n",
        "\n",
        "# Function to summarize data\n",
        "def summarize_data(df):\n",
        "    print(f\"Number of records: {len(df)}\")\n",
        "    print(f\"Number of columns: {len(df.columns)}\")\n",
        "    print(f\"Columns: {list(df.columns)}\\n\")\n",
        "\n",
        "# Main function to let the user choose\n",
        "def main():\n",
        "    df_modified = add_exact_date_report_column(df)\n",
        "    df_modified = split_reporting_officer(df_modified)\n",
        "    choice = input(\"Do you want to convert the CSV file to JSON or SQL Database? Enter 'JSON' or 'SQL': \")\n",
        "\n",
        "    if choice == 'JSON':\n",
        "        convert_csv_to_json(df, file_path)\n",
        "    elif choice == 'SQL':\n",
        "        convert_csv_to_sql(df, file_path)\n",
        "    else:\n",
        "        print(\"Invalid choice! Please enter 'JSON' or 'SQL'.\") # Prompting user to put only these two options\n",
        "\n",
        "    print(\"Pre-modified Data Summary:\")\n",
        "    summarize_data(dfbefore)\n",
        "    print(\"Post-modified Summary:\")\n",
        "    summarize_data(df_modified)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fICrAwQ8b7KN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29626bab-2ec5-4714-abea-965841dd23e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you want to convert the CSV file to JSON or SQL Database? Enter 'JSON' or 'SQL': JSON\n",
            "CSV file converted to JSON and saved as /content/DS2002F24/dataproject1/Crime_Data.json\n",
            "Pre-modified Data Summary:\n",
            "Number of records: 25111\n",
            "Number of columns: 9\n",
            "Columns: ['RecordID', 'Offense', 'IncidentID', 'BlockNumber', 'StreetName', 'Agency', 'DateReported', 'HourReported', 'ReportingOfficer']\n",
            "\n",
            "Post-modified Summary:\n",
            "Number of records: 25111\n",
            "Number of columns: 12\n",
            "Columns: ['RecordID', 'Offense', 'IncidentID', 'BlockNumber', 'StreetName', 'Agency', 'DateReported', 'HourReported', 'ReportingOfficer', 'ExactDateReport', 'OfficerLastName', 'OfficerFirstName']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import sqlite3\n",
        "import os\n",
        "\n",
        "# Set file path (hardcoded)\n",
        "file_path = '/content/DS2002F24/dataproject1/sportscarprice.json'\n",
        "\n",
        "# Function to delete 'Engine Size (L)' column from a JSON file\n",
        "def delete_engine_size_column_json(file_path):\n",
        "    try:\n",
        "        # Load the JSON file into a DataFrame\n",
        "        with open(file_path, 'r') as json_file:\n",
        "            json_data = json.load(json_file)\n",
        "        df = pd.DataFrame(json_data)\n",
        "\n",
        "        # Summary before processing (Ingestion Summary)\n",
        "        print(\"Ingestion Summary:\")\n",
        "        print(f\"Number of records: {len(df)}\")\n",
        "        print(f\"Number of columns: {len(df.columns)}\")\n",
        "\n",
        "        # Check if the column 'Engine Size (L)' exists, and drop it\n",
        "        if 'Engine Size (L)' in df.columns:\n",
        "            df = df.drop(columns=['Engine Size (L)'])\n",
        "\n",
        "        # Save the updated DataFrame back to JSON\n",
        "        new_file_path = file_path.replace('.json', '_updated.json')\n",
        "        df.to_json(new_file_path, orient='records', indent=4)\n",
        "\n",
        "        # Summary after processing (Post-Processing Summary)\n",
        "        print(\"\\nPost-Processing Summary:\")\n",
        "        print(f\"Number of records: {len(df)}\")\n",
        "        print(f\"Number of columns: {len(df.columns)}\")\n",
        "\n",
        "        return df  # Return the modified DataFrame (optional)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing JSON file: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to convert DataFrame to CSV\n",
        "def convert_json_to_csv(df, file_path):\n",
        "    try:\n",
        "        csv_file_path = file_path.replace('.json', '.csv')\n",
        "        df.to_csv(csv_file_path, index=False)\n",
        "        print(f\"JSON file converted to CSV and saved as {csv_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting JSON to CSV: {e}\")\n",
        "\n",
        "# Function to convert DataFrame to SQL\n",
        "def convert_json_to_sql(df, file_path, db_name='json_data.db', table_name='json_records'):\n",
        "    try:\n",
        "        # Connecting to the SQLite database\n",
        "        conn = sqlite3.connect(db_name)\n",
        "\n",
        "        # Write the dataframe to the SQL database\n",
        "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
        "        conn.close()\n",
        "        print(f\"JSON file converted to SQL and saved in {db_name} database, table: {table_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting JSON to SQL: {e}\")\n",
        "\n",
        "# Main function to allow user to choose conversion type\n",
        "def main():\n",
        "    # Delete Engine Size (L) column\n",
        "    df_modified = delete_engine_size_column_json(file_path)\n",
        "\n",
        "    if df_modified is not None:\n",
        "        choice = input(\"Do you want to convert the modified JSON file to CSV or SQL Database? Enter 'CSV' or 'SQL': \") # Prompting user choice\n",
        "\n",
        "        if choice.upper() == 'CSV':\n",
        "            convert_json_to_csv(df_modified, file_path)\n",
        "        elif choice.upper() == 'SQL':\n",
        "            convert_json_to_sql(df_modified, file_path)\n",
        "        else:\n",
        "            print(\"Invalid choice! Please enter 'CSV' or 'SQL'.\")\n",
        "    else:\n",
        "        print(\"Unable to load the JSON file. Please check the file format.\") # If it can not open\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSBcwUGbXDBR",
        "outputId": "5427a73a-2ae2-47a3-be18-d6c30a97cfd6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingestion Summary:\n",
            "Number of records: 1007\n",
            "Number of columns: 8\n",
            "\n",
            "Post-Processing Summary:\n",
            "Number of records: 1007\n",
            "Number of columns: 7\n",
            "Do you want to convert the modified JSON file to CSV or SQL Database? Enter 'CSV' or 'SQL': SQL\n",
            "JSON file converted to SQL and saved in json_data.db database, table: json_records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reflection**\n",
        "During this project, one of the main challenges I faced was managing the transformation between different file formats. Since the process of switching the input file into a JSON and SQL are different, I had to create functions that would execute based on conditions. I also had to create handle the editing/modification of the files with the data manipualtion differently. Adding/removing files from JSON and SQL are different, but fortunately CSV was straight forward to do and was similar to SQL. Specifically with these files, the transformation complex JSON structures into the relational format proved by the user seemed to be more difficult than I thought. Additionally, managing potential errors that could come into effect like missing columns or formatting issues required me to put except conditions to handle these errors allow for a smooth execution of the code. Also, I had to make sure the user put in only 2 options that were provided and if they didn't I had to not let the code execute and prompt them to put in a correct choice.\n",
        "\n",
        "Despite these challenges, some aspects of the project were easier than I expected. Using **pandas** to handle data manipulation turned out to be pretty straightforward for csv files, and similarly with editing SQL files. Also, setting up the user input and flow control in the script was manageable to get the function to continue to use this. With this process, I allowed the user to choose between CSV or SQL output which was easy to ask, however like previously stated, to actually convert it was a little tricky. Finally, the easiest part was saving the file back to the system or saving it as a sql database. Both of these proved to be simple.\n",
        "\n",
        "CONTINUE HERE\n",
        "\n",
        "\n",
        "What proved to be harder than expected was ensuring smooth error handling across various file formats. Anticipating cases where the `\"Engine Size (L)\"` column might not exist, or when the JSON structure was inconsistent, required thorough validation and error messaging to ensure the pipeline handled these edge cases gracefully. Another unexpectedly complex aspect was the data conversion into SQL. SQLite databases, while convenient, have certain limitations with respect to data types, which made ensuring data integrity during conversion a bit more complicated than anticipated.\n",
        "\n",
        "Overall, the utility developed for this project offers substantial value for future data tasks. Many data projects involve working with diverse data sources—whether they come from APIs, CSV files, or relational databases. Having a flexible ETL processor like this one can significantly streamline workflows by automating common tasks like data ingestion, format conversion, and preprocessing. This utility could easily be extended to handle more complex workflows, such as integrating data from multiple sources or adding features for more advanced data cleaning. Furthermore, in future team-based projects, this processor’s modularity, combined with version control through platforms like GitHub, would enhance collaboration and allow for incremental improvements. Overall, this ETL processor provides a solid foundation for transforming, cleaning, and converting data, making it highly useful for a wide range of future data science projects.\n"
      ],
      "metadata": {
        "id": "TqvFj1iflwF2"
      }
    }
  ]
}